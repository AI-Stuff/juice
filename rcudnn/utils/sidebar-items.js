initSidebarItems({"enum":[["DataType","Defines the available data types for the CUDA cuDNN data representation."]],"struct":[["ActivationConfig","Provides a convenient interface to access cuDNN's Activation Descriptor."],["ConvolutionConfig","Provides a convenient interface to access cuDNN's convolution parameters, `algo` and `workspace` and `workspace_size_in_bytes`."],["DropoutConfig","Provides a convenient interface to access cuDNN's Dropout Descriptor."],["NormalizationConfig","Provides a convenient interface to access cuDNN's Normalization (LRN) Descriptor."],["PoolingConfig","Provides a convenient interface to access cuDNN's Pooling Descriptor."],["RnnConfig","Provides an interfaces for CUDNN's Rnn Descriptor # Arguments * `rnn_desc` Previously created descriptor * `hidden_size` Size of the hidden layer * `num_layers` Number of layers * `dropout_desc` Descriptor to a previously created & initialized dropout descriptor, applied between layers.  * `input_mode` Specifies behaviour at the input to the first layer * `direction_mode` Specifies the recurrence pattern - i.e bidirectional * `rnn_mode` Type of network used in routines ForwardInference, ForwardTraining, BackwardData, BackwardWeights. Can be ReLU, tanh, LSTM (Long Short Term Memory), or GRU (Gated Recurrent Unit). * `algo` - Only required in v6 implementation FIXME: Should this be checked in compilation? * `data_type` Math Precision - default f32"],["ScalParams","Provides a convenient interface for cuDNN's scaling parameters `alpha` and `beta`."]],"trait":[["DataTypeInfo","CuDnn type info for generic use."]]});